{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MweinbergUmass/Teach_Dimred/blob/main/Collab_Python/tsnedisplay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e62Gxf-v5im"
      },
      "source": [
        "\n",
        "\n",
        "In order to understand what Tsne does and how it works we should start by considering the concept of a distance matrix. Consider a matrix X, each of it's rows is a list of numbers that somehow describe the data. For instance, in the first data set we are going to consider, the fisher iris dataset, there are four columns: Sepal length, Sepal width, Petal length, Petal width.\n",
        "\n",
        "\n",
        "Put another way, someone at some point sat down, grabbed an iris (flower), recorded it's sepal length, width, etc... and now I have them in the format of one flower per row. The reason I'm harping on this is because thinking of rows as going together is deeply important for dimensionality reduction and data science broadly.\n",
        "\n",
        "\n",
        "Now, back to the distance matrix X:\n",
        "\n",
        "\n",
        "When I say distance all I mean is, you take datapoint x, and datapoint y, and find the euclidean distance between them.\n",
        "\n",
        "\n",
        "Note (we're working in two dimensions for now)\n",
        "\n",
        "\n",
        "In math:\n",
        "\n",
        "\n",
        "$$d(x,y)=\\sqrt{(x_2 -x_1 )^2 +(y_2 -y_1 )^2 }$$\n",
        "\n",
        "Now in Python:\n",
        "\n",
        "<pre>\n",
        "def d(x, y):\n",
        "    return np.sqrt((x[1] - x[0])**2 + (y[1] - y[0])**2)\n",
        "</pre>\n",
        "\n",
        "Now let's get going. We're going to walk through how Tsne does what it does with Python. If you're new to google collab all you have to do should be to hit the run button on the left of each cell and then play with the sliders to see how they update the plots."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell, it's the imports.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm, t\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import cm, colormaps\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m4I6_Q2hwcxm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title These are the functions we will be using.\n",
        "def compute_entropy(current_row, Sigma):\n",
        "    num = np.exp(-current_row / (2 * Sigma**2))\n",
        "    num[num == np.max(num)] = 0\n",
        "    thisP = num / np.sum(num)\n",
        "    H = -np.sum(thisP * np.log2(thisP + np.finfo(float).eps))\n",
        "    return H, thisP\n",
        "\n",
        "def binary_search_sigma(clusters, perplexity, max_iter, tolerance):\n",
        "    target_entropy = np.log(perplexity)\n",
        "    distances = squareform(pdist(clusters))**2\n",
        "    sigmas = []\n",
        "    for i in range(clusters.shape[0]):\n",
        "        current_row = distances[i, :]\n",
        "        sigmamin = np.finfo(float).eps\n",
        "        sigmamax = np.max(current_row)\n",
        "        tries = 0\n",
        "        Sigma = (sigmamin + sigmamax) / 2\n",
        "        H, _ = compute_entropy(current_row, Sigma)\n",
        "        while tries != max_iter and abs(H - target_entropy) > tolerance:\n",
        "            if H > target_entropy:\n",
        "                sigmamax = Sigma\n",
        "            elif H < target_entropy:\n",
        "                sigmamin = Sigma\n",
        "            Sigma = (sigmamin + sigmamax) / 2\n",
        "            H, _ = compute_entropy(current_row, Sigma)\n",
        "            tries += 1\n",
        "        sigmas.append(Sigma)\n",
        "    return np.array(sigmas)\n",
        "\n",
        "\n",
        "def compute_KLD_display(P, Q):\n",
        "    KLD = np.sum(P * np.log(P / Q))\n",
        "    return KLD\n",
        "\n",
        "def gradientdescentvis(grid_res=0.75, x_start=-10, y_start=10, alpha=0.01, num_iterations=100, scale_factor=0.2,\n",
        "                       sin_multiplier=1, sin_amplitude=5, x_bound=15, y_bound=15, plot_gradient_field=False,\n",
        "                       gradient_field_scale_factor=0.05):\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(-x_bound, x_bound + grid_res, grid_res),\n",
        "                       np.arange(-y_bound, y_bound + grid_res, grid_res))\n",
        "    z = x**2 + y**2 + sin_amplitude*np.sin(sin_multiplier*x) + sin_amplitude*np.sin(sin_multiplier*y)\n",
        "\n",
        "    x_current = x_start\n",
        "    y_current = y_start\n",
        "    z_current = x_current**2 + y_current**2 + sin_amplitude*np.sin(sin_multiplier*x_current) + sin_amplitude*np.sin(sin_multiplier*y_current)\n",
        "\n",
        "    offset = 10\n",
        "\n",
        "    x_trajectory, y_trajectory, z_trajectory = [x_current], [y_current], [z_current + offset]\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        x_previous = x_current\n",
        "        y_previous = y_current\n",
        "        z_previous = z_current\n",
        "\n",
        "        dx = 2*x_current + sin_multiplier * sin_amplitude * np.cos(sin_multiplier * x_current)\n",
        "        dy = 2*y_current + sin_multiplier * sin_amplitude * np.cos(sin_multiplier * y_current)\n",
        "\n",
        "        x_current = x_current - alpha*dx\n",
        "        y_current = y_current - alpha*dy\n",
        "        z_current = x_current**2 + y_current**2 + sin_amplitude*np.sin(sin_multiplier*x_current) + sin_amplitude*np.sin(sin_multiplier*y_current)\n",
        "\n",
        "        x_trajectory.append(x_current)\n",
        "        y_trajectory.append(y_current)\n",
        "        z_trajectory.append(z_current + offset)\n",
        "\n",
        "    return x, y, z, x_trajectory, y_trajectory, z_trajectory\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_gradient_field(grid_res=0.75, sin_multiplier=1, sin_amplitude=5, x_bound=15, y_bound=15, gradient_field_scale_factor=0.05):\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(-x_bound, x_bound + grid_res, grid_res),\n",
        "                       np.arange(-y_bound, y_bound + grid_res, grid_res))\n",
        "\n",
        "    z = x**2 + y**2 + sin_amplitude*np.sin(sin_multiplier*x) + sin_amplitude*np.sin(sin_multiplier*y)\n",
        "\n",
        "    # Calculate the gradient\n",
        "    dx = 2*x + sin_multiplier * sin_amplitude * np.cos(sin_multiplier * x)\n",
        "    dy = 2*y + sin_multiplier * sin_amplitude * np.cos(sin_multiplier * y)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.contourf(x, y, z, 50, cmap='viridis')  # Plotting the function\n",
        "    ax.quiver(x, y, -gradient_field_scale_factor * dx, -gradient_field_scale_factor * dy, scale=25)  # Plotting the gradient vectors\n",
        "\n",
        "    ax.set_xlabel('X-axis')\n",
        "    ax.set_ylabel('Y-axis')\n",
        "    ax.set_title('Gradient Field')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Function to update the t-SNE visualization\n",
        "def update_tsne(perplexity=30, exaggeration=3, alpha=190):\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=alpha, early_exaggeration=exaggeration, random_state=0)\n",
        "    y_tsne = tsne.fit_transform(X)\n",
        "    cmap = colormaps['viridis']\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_tsne[:, 0], y_tsne[:, 1], c=y, cmap=cmap, marker='o')\n",
        "    plt.title(f\"t-SNE Visualization (Perplexity={perplexity}, Exaggeration={exaggeration}, Alpha={alpha})\")\n",
        "    plt.colorbar(ticks=np.unique(y), label='Class')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "9JzDR6sMAZ6g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by plotting two points each defined by two dimensions. Run this cell to see these points."
      ],
      "metadata": {
        "id": "sML22zoVP8HC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPS8p6sov5io"
      },
      "source": [
        "\n",
        "# Define two points\n",
        "p = [1, 2]\n",
        "q = [4, 6]\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the points\n",
        "plt.plot(p[0], p[1], 'ro', markersize=10, linewidth=2)  # 'ro' indicates red color, 'o' marker\n",
        "plt.plot(q[0], q[1], 'bo', markersize=10, linewidth=2)  # 'bo' indicates blue color, 'o' marker\n",
        "\n",
        "# Set x and y axis limits\n",
        "plt.xlim([0, 8])\n",
        "plt.ylim([0, 8])\n",
        "\n",
        "# Draw the lines\n",
        "plt.plot([p[0], q[0]], [p[1], p[1]], 'k-')  # Horizontal line\n",
        "plt.plot([q[0], q[0]], [p[1], q[1]], 'k-')  # Vertical line\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at what I mean by distance"
      ],
      "metadata": {
        "id": "u4oHtz_xQIuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlZj6NOHv5ip"
      },
      "source": [
        "# Plot the line representing the distance between p and q\n",
        "plt.figure()\n",
        "plt.plot([p[0], q[0]], [p[1], q[1]], color='k', linewidth=1.5)\n",
        "\n",
        "plt.plot(p[0], p[1], 'ro', markersize=10, linewidth=2)\n",
        "plt.plot(q[0], q[1], 'bo', markersize=10, linewidth=2)\n",
        "\n",
        "plt.plot(p[0], p[1], 'ro', markersize=10, linewidth=2)\n",
        "plt.plot(q[0], q[1], 'bo', markersize=10, linewidth=2)\n",
        "plt.plot([p[0], q[0]], [p[1], p[1]], 'k-')\n",
        "plt.plot([q[0], q[0]], [p[1], q[1]], 'k-')\n",
        "\n",
        "plt.xlim([0, 8])\n",
        "plt.ylim([0, 8])\n",
        "\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See, it's just Pythagoras!"
      ],
      "metadata": {
        "id": "sVSYvQucQOyT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWjgaUShv5ip"
      },
      "source": [
        "\n",
        "An important thing to note, is that because we're doing rows of data (vectors), there is a slight change to the formula for distances, but not much and it is still the same idea.\n",
        "\n",
        "\n",
        "it's given by:\n",
        "  $\\sqrt{\\sum_{i = 1}^{n}({x_i - y_i)^2}}$\n",
        "\n",
        "so if we have vectors:\n",
        "\n",
        "\n",
        "$$\\left\\lbrack \\begin{array}{ccc} 3 & 4 & 4 \\end{array}\\right\\rbrack$$\n",
        "\n",
        "$$\\left\\lbrack \\begin{array}{ccc} 4 & 5 & 1 \\end{array}\\right\\rbrack$$\n",
        "\n",
        "$$\\begin{array}{l} d(p,q)=\\sqrt{(3-4)^2 +(4-5)^2 +(4-1)^2 }\\\\ =\\sqrt{1+1+9}=\\sqrt{11}. \\end{array}$$\n",
        "\n",
        "\n",
        "in code now\n",
        "\n",
        "<pre>\n",
        "def vector_distance(p, q):\n",
        "    distance = 0\n",
        "    for pi, qi in zip(p, q):\n",
        "        distance += (pi - qi) ** 2\n",
        "    return distance ** 0.5\n",
        "\n",
        "# this is just for illustrative purposes.\n",
        "# In practice you should use dist = numpy.linalg.norm(a-b)\n",
        "</pre>\n",
        "\n",
        "Boom, Now we are officically ready for the concept of a distance matrix!\n",
        "\n",
        "\n",
        "A distance matrix is defined such that the ith row captures how far away it is from all the different datapoints.\n",
        "\n",
        "\n",
        "So the ith row in the jth column tells you how far away data point i is from data point j.\n",
        "\n",
        "\n",
        "In code:\n",
        "\n",
        "<pre>\n",
        "def dm(p):\n",
        "    data_points, dim = p.shape\n",
        "    distance_matrix = np.zeros((data_points, data_points))\n",
        "    for i in range(data_points):\n",
        "        for j in range(data_points):\n",
        "            if i != j:\n",
        "                sum_square_diff = sum([(p[i, k] - p[j, k])**2 for k in range(dim)])\n",
        "                distance_matrix[i, j] = np.sqrt(sum_square_diff)\n",
        "    return distance_matrix\n",
        "</pre>\n",
        "\n",
        "I should really note this, this code is for illustrative purposes, in practice, this can be succinctyly written with (using scipy):\n",
        "\n",
        "<pre>\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "distance_matrix = squareform(pdist(p));\n",
        "</pre>\n",
        "\n",
        "The concept of a distance between two points can be concicely written with these bars:\n",
        "\n",
        "\n",
        "$$\\frac{||p-q||}{}$$\n",
        "\n",
        "reads: the norm of the difference (distance) between vectors p and q.\n",
        "\n",
        "\n",
        "Okay now that we have our distance matrix it's time to define our probability distribution matrix.\n",
        "\n",
        "\n",
        "I like to think of a probability distribtuion as a function that takes in some parameters and spits out a prob\n",
        "\n",
        "\n",
        "so instead of having a distance matrix we will have a probability distirbution matrix... It's the exact same prcocess just a different formula to plug your data points into.\n",
        "\n",
        "\n",
        "Well, there is one slight difference worth talking about. There are two distributions we're going to use, a t and a normal or gaussian. We can focus on the gaussian for now. The gaussian takes in two parameters, a mean and a standard deviation. Once you give it those two parameters it becomes a well defined probability distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at what happens when we change the mean of the Gaussian and hold the standard deviation constant."
      ],
      "metadata": {
        "id": "M_wcY2PT2gxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3er32Ghv5ip"
      },
      "source": [
        "X = np.arange(-10, 10.01, 0.01)\n",
        "Means = [0, -1, -2, 1, 1]\n",
        "\n",
        "for mean_val in Means:\n",
        "    gaussian = norm.pdf(X, mean_val, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(X, gaussian)\n",
        "    plt.title(f'A Gaussian with a mean of {mean_val}')\n",
        "    plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpznWf-9v5iq"
      },
      "source": [
        "\n",
        "Lets take a look at what happens when do the reverse; hold the mean constant and change the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkxOcNT1v5iq"
      },
      "source": [
        "X = np.arange(-20, 20.01, 0.01)\n",
        "STDs = [1, 2, 4, 6]\n",
        "for std in STDs:\n",
        "    gaussian = norm.pdf(X, 0, std)\n",
        "    plt.figure()\n",
        "    plt.plot(X, gaussian)\n",
        "    plt.title(f'A Gaussian with a mean of 0 and a standard deviation of {std}')\n",
        "    plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7vw7EYwv5iq"
      },
      "source": [
        "\n",
        "\n",
        "So there are two parameters we need for our high dimensional probability matrix P, we need the mean and the standard deviation. The mean is easy. For each data point we set it to be the mean of the gaussian.\n",
        "\n",
        "\n",
        "The standard deviation is less easy, but we can get back to it.\n",
        "\n",
        "\n",
        "Now lets look at how we define our high dimensional probability distribution matrix\n",
        "\n",
        "\n",
        "$$p_{j|i} =\\frac{\\exp \\left(-\\frac{||x_i -x_j ||^2 }{2\\sigma_i^2 }\\right)}{\\sum_{k\\not= i} \\exp \\left(-\\frac{||x_i -x_k ||^2 }{2\\sigma_i^2 }\\right)}$$\n",
        "\n",
        "(Hint: that i below the sigma is meant to be there but dont worry about it too much for now)\n",
        "\n",
        "What this is saying is we take each data point in our distance matrix, set it to be the mean and ask, how likely are each of the other datapoints?\n",
        "\n",
        "\n",
        "Points that are far will get a low value and points that are close will get a high value.\n",
        "\n",
        "\n",
        "Lets take a look at the numerator and see how it behaves with different distances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE1cOlzyv5iq"
      },
      "source": [
        "# Gaussian Curve\n",
        "sigma = 1\n",
        "denom1 = 2 * sigma**2\n",
        "\n",
        "def numeratorfun(dist):\n",
        "    return np.exp(-(dist**2) / denom1)\n",
        "\n",
        "dists = np.arange(-5, 5.01, 0.01)\n",
        "pji_notnormalize = [numeratorfun(dist) for dist in dists]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(dists, pji_notnormalize)\n",
        "plt.title('Gaussian Distribution with Mean 0')\n",
        "plt.xlabel('Distance from Mean')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9jT39VDv5iq"
      },
      "source": [
        "Ah out pops the Gaussian. The denominator is there just to normalize the probability distribution. So each row is a conditional distribution which tells you how likely each other data point is as the neighbor of the row.\n",
        "\n",
        "Now for a trick...\n",
        "\n",
        "This is hard for me to motivate exactly. It helps to think in terms of what we are trying to do here. We want to capture the structures that exist in our data. In particular we're interested in the points that are more similar than other points, we're less interested in the global structure (why? Tradeoffs in reducing dimensions...) and more interested in the neighborhoods that each point is in.\n",
        "\n",
        "\n",
        "One thing you could do, is define a standard deviation yourself... but how would you do that? You want to account for the fact that different data points should consider the same number of neighbors, but picking just one standard deviation for all of your data means that some points will get very few neighbors and others will get lots of neighbors. Tsne tries to capture the local structure of the data and this would be entirely removed if we used just one standard deviation.\n",
        "\n",
        "Let's take a look at what I mean. First lets look at what happends with 5 random points and 1 uniform standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROvvnJelv5iq"
      },
      "source": [
        "def single_std(sigma=0.5):\n",
        "  # Scatter plot with single sigma\n",
        "  np.random.seed(0)  # For reproducibility\n",
        "  denseCluster = np.random.randn(5, 2)\n",
        "  sparseCluster = np.random.randn(5, 2) * 3 + [5, 0]\n",
        "  clusters = np.vstack([denseCluster, sparseCluster])\n",
        "\n",
        "  plt.figure()\n",
        "  plt.scatter(denseCluster[:, 0], denseCluster[:, 1], s=50, color='b', label='Dense Cluster')\n",
        "  plt.scatter(sparseCluster[:, 0], sparseCluster[:, 1], s=50, color='r', label='Sparse Cluster')\n",
        "\n",
        "  for point in clusters:\n",
        "      circle = plt.Circle((point[0], point[1]), sigma, color='k', linestyle='-.', linewidth=1.5, fill=False)\n",
        "      plt.gca().add_patch(circle)\n",
        "\n",
        "  plt.title('Data Clusters with Single Sigma')\n",
        "  plt.xlabel('X-axis')\n",
        "  plt.ylabel('Y-axis')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "widgets.interactive(single_std, sigma=(0.5, 5, 0.01))\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What ends up happening is that some points have no neighbors for a single Ïƒ.\n",
        "Also, if a point p only has one neighbor q then they will be smooshed in the low dimensional representation even if they are really far away in the high dimensional data. The other point q might have p as a very unlikely neighbor, this will lead to significant problems in how we end up finding the low dimensional representation (the cost function).\n",
        "\n",
        "Now let's take a look at what happens when we define a notion of nearest neighbors using a parameter called perplexity."
      ],
      "metadata": {
        "id": "nf4x99nsWIDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hsqmf42v5iq"
      },
      "source": [
        "\n",
        "def plot_clusters(perplexity=24):\n",
        "    max_iter = 50\n",
        "    tolerance = 1e-8\n",
        "    sigmas = binary_search_sigma(clusters, perplexity, max_iter, tolerance)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.scatter(denseCluster[:, 0], denseCluster[:, 1], s=100, color='b', label='Dense Cluster')\n",
        "    plt.scatter(sparseCluster[:, 0], sparseCluster[:, 1], s=100, color='r', label='Sparse Cluster')\n",
        "\n",
        "    for i, point in enumerate(clusters):\n",
        "        circle = plt.Circle((point[0], point[1]), sigmas[i], color='k', linestyle='-.', linewidth=1.5, fill=False)\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    plt.title(f'Data Clusters with Perplexity-based Sigma (Perplexity = {perplexity})')\n",
        "    plt.xlabel('X-axis')\n",
        "    plt.ylabel('Y-axis')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim([-10, 10])\n",
        "    plt.ylim([-10, 10])\n",
        "    plt.show()\n",
        "\n",
        "# Create an interactive widget\n",
        "denseCluster = np.random.randn(5, 2)\n",
        "sparseCluster = np.random.randn(5, 2) * 3 + [5, 0]\n",
        "clusters = np.vstack([denseCluster, sparseCluster])\n",
        "widgets.interactive(plot_clusters, perplexity=(2, 20, 1))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxgH6pTpv5ir"
      },
      "source": [
        "\n",
        "\n",
        "Perplexity is borrowed from information theory and is defined as:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$H=-\\sum_j p_{j|i} \\log_2 p_{j|i}$$\n",
        "\n",
        "$$\\textrm{Perplexity}=2^H$$\n",
        "\n",
        "H is the entropy, which measures the information of a discrete (it can do continuous too but it uses an integral instead of a sum) random variable. It does this by summing all the log probabibilities, the \"suprisal\" (its called  suprisal because improbale events have a high value) then scaling by the probability to determine how much it will contribute to the overall sum (more probable events will contribute more to the sum).\n",
        "Entropy has the important property that when events are equally likely it is maximized, in otherwords, is the most uncertain it could be.\n",
        "\n",
        "\n",
        "when you take $2^H$ you're effectively counting the total number of effective events.\n",
        "\n",
        "\n",
        "lets do an example with a coin, C:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2zOVjG_v5ir"
      },
      "source": [
        "Coin = [0.5,0.5];\n",
        "entropy = 0;\n",
        "for coin in Coin:\n",
        "    entropy += coin * np.log2(coin);\n",
        "entropy = -entropy\n",
        "print(entropy)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmgp3tJUv5ir"
      },
      "source": [
        "\n",
        "\n",
        "So the entropy of a fair coin toss is one, in other words,  you get one bit of information when a coin is tossed.\n",
        "\n",
        "\n",
        "Now lets look at what happens when take the perplexity:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fF9BtSOv5ir"
      },
      "source": [
        "perplexity = 2**entropy\n",
        "print(perplexity)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrrj_EnLv5ir"
      },
      "source": [
        "\n",
        "Ah... two possible outcomes.\n",
        "\n",
        "\n",
        "So to bring it back, what tsne does is defines a user input perplexity, which roughly corresponds to the number of neighbors to consider for each point.\n",
        "\n",
        "\n",
        "It does this by using a binary search to find a sigma for each datapoint such that when you compute the entropy of that distribution it matches the entropy given by the input perplexity. perplexity = log(entropy).\n",
        "\n",
        "\n",
        "Here it is in code:\n",
        "\n",
        "<pre>\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def binary_search_sigma(Data, perplexity, max_iter, tolerance):\n",
        "    # max_iter = how many times to cut the search space in half\n",
        "    # tolerance = how close do the target entropy and the distribution's entropy need to be.\n",
        "    target_entropy = np.log(perplexity)\n",
        "    distances = squareform(pdist(Data)) ** 2  # create distance matrix\n",
        "    sigmas = np.zeros(Data.shape[0])  # initialize sigmas array\n",
        "    for i in range(Data.shape[0]):  # for the number of data points (rows)\n",
        "        current_row = distances[i, :]  # take the current data point.\n",
        "        sigmamin = np.finfo(float).eps  # initialize the minimum possible sigma\n",
        "        sigmamax = np.max(current_row)  # sigma can't be greater than the distance between current and furthest point\n",
        "        tries = 0\n",
        "        Sigma = (sigmamin + sigmamax) / 2  # compute the initial value for sigma\n",
        "        entropy, _ = compute_entropy(current_row, Sigma)  # find the entropy of the current distribution\n",
        "        while tries != max_iter and abs(entropy - target_entropy) > tolerance:\n",
        "            # While we haven't tried the max number of times and the difference between entropy and target entropy is greater than a tolerance.\n",
        "            if entropy > target_entropy:\n",
        "                sigmamax = Sigma  # cut search space in half from the top\n",
        "            elif entropy < target_entropy:\n",
        "                sigmamin = Sigma  # cut search space in half from the bottom\n",
        "            Sigma = (sigmamin + sigmamax) / 2  # compute new sigma\n",
        "            entropy, _ = compute_entropy(current_row, Sigma)  # compute new entropy\n",
        "            tries += 1\n",
        "        sigmas[i] = Sigma\n",
        "    return sigmas\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell to see what it looks like when we're looking for a point on a function: $F(X) = X^2$"
      ],
      "metadata": {
        "id": "lgB9hQ2sYkZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgYESEKtv5ir"
      },
      "source": [
        "X = np.arange(-10, 10.1, 0.1)\n",
        "Yfun = lambda X: X**2\n",
        "Y = Yfun(X)\n",
        "point_to_find = Y[10]\n",
        "max_iter = 50\n",
        "\n",
        "def plot_binary_search(step=0):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(X, Y, label='Function')\n",
        "    plt.plot(X[10], point_to_find, 'ro', linewidth=2, label='Point to Find')\n",
        "\n",
        "    # Initializing binary search parameters\n",
        "    min_x = np.min(X)\n",
        "    max_x = np.max(X)\n",
        "    tolerance = 1e-5\n",
        "    guess = (min_x + max_x) / 2\n",
        "    y_hat = Yfun(guess)\n",
        "    tries = 0\n",
        "\n",
        "    while tries <= step and tries < max_iter:\n",
        "        if abs(y_hat - point_to_find) <= tolerance:\n",
        "            break\n",
        "        plt.plot(guess, y_hat, 'bx', linewidth=2, label='Guess' if tries == 0 else \"\")\n",
        "        if y_hat > point_to_find:\n",
        "            min_x = guess\n",
        "        elif y_hat < point_to_find:\n",
        "            max_x = guess\n",
        "        guess = (min_x + max_x) / 2\n",
        "        y_hat = Yfun(guess)\n",
        "        tries += 1\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "widgets.interactive(plot_binary_search, step=(0, max_iter-1))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMTN8DTv5ir"
      },
      "source": [
        "\n",
        "Okay now we have a matrix, P(i|j), which represents the probability of point i picking point j as its neighbor. Unfortunately, these might not be symmetric, that is, the probability of picking point i given point j might not be the same as the probability of picking point j given point i. This is a problem because when we project into a lower dimension we want to have an interpetable relationship, and if we did it as so, with asymetry, when looking at the projection one would have to consider the direction of relationship.\n",
        "\n",
        "(Big plus is simpler gradient)\n",
        "\n",
        "The simple part here is actually symmetrizing which is given by:\n",
        "\n",
        "\n",
        "$$P_{ij} =\\frac{p_{j|i} +p_{i|j} }{2N}$$\n",
        "\n",
        "\n",
        "The way to read this is: take the average of each point and divide by N (the number of data points you have). You divide by N to ensure it's a valid probability distribution over all the points, that is, it sums to 1.\n",
        "\n",
        "\n",
        "Okay I'm going to recap now.\n",
        "\n",
        "\n",
        "Step 1: Create a distance matrix (D) from our data (X)\n",
        "\n",
        "\n",
        "Step 2: Compute pairwise affinitiy matrix (Pij) from X using a gaussian function with each data point as the center.\n",
        "\n",
        "\n",
        "Step 2a: Find the standard deviation for each row, datapoint, using a binary search given the user input, perplexity.\n",
        "\n",
        "\n",
        "Step 2b: Symmetrize our distribution.\n",
        "\n",
        "\n",
        "Here is a sketch of the remainder of the steps:\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Initialize low dimenisonal embedding (Y) from our data. All that means is that if our data originally had N row (datapoints) and M columns (dimensions, i.e, length, width, height, color, etc...) we want our \"embedding\" to have the same number of datapoints but with less columns, 2 or 3.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Perform steps 1 and 2 on Y such that we are left with a matrix (Qij) which represents the low dimensional pairwise affinities assuming a               students T distribution.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Compute a measure of difference between the two distributions (Kullback Leibler Divergence, KLD).\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Minimize that difference between Qij and Pij by updating Y using gradient descent.\n",
        "\n",
        "\n",
        "Initializing an embedding is easy, it looks like this:\n",
        "\n",
        "<pre>\n",
        " Y = np.randn(N, 2);\n",
        "</pre>\n",
        "\n",
        "Then we compute a distance matrix:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<pre>\n",
        "Distance = squareform(pdist(Y).^2\n",
        "</pre>\n",
        "\n",
        "Then we compute the pairwisee affinities Qij. The one difference here is that we use students T distribution with one DF, or Cauchy, to model our data instead of a Gaussian. We do this because of it's heavier tails. In other words, points that are at a moderate distance from each other in the High D space, will not have enough room in the low d space to spread out if we use a gaussian... that is, their distances will be way too high.\n",
        "\n",
        "\n",
        "To show you what I mean:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsHrrQfyv5ir"
      },
      "source": [
        "X = np.arange(-20, 20.01, 0.01)\n",
        "DF = 1\n",
        "sigma = 1\n",
        "Mu = 0\n",
        "T_distribution1 = t.pdf(X, DF)\n",
        "Gaussian = norm.pdf(X, Mu, sigma)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, T_distribution1, label='T Distribution: DF = 1')\n",
        "plt.plot(X, Gaussian, label='Gaussian: Mu = 0, Sigma = 1')\n",
        "plt.title('Wider Tails of the T Distribution')\n",
        "plt.legend(loc='upper right', fontsize='small')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJOa7E0Fv5is"
      },
      "source": [
        "\n",
        "A  T disttibution takes in 1 parameter, the degrees of freedom, in TSNE we just use a value of 1.\n",
        "\n",
        "\n",
        "That gives us this way of computing our pairwise affinities in the low dimensional space:\n",
        "\n",
        "\n",
        "$$q_{ij} =\\frac{\\exp (-||y_i -y_j ||^2 )}{\\sum_{k\\not= l} \\exp (-||y_k -y_l ||^2 )}$$\n",
        "\n",
        "In code:\n",
        "\n",
        "<pre>\n",
        "def compute_lowD_P_affin(Y):\n",
        "    low_D = squareform(pdist(Y))**2  # distance matrix\n",
        "    num = (1 + low_D)**-1  # numerator\n",
        "    np.fill_diagonal(num, 0)  # set diagonals to zero\n",
        "    qij = num / (np.sum(num) + np.finfo(float).eps)  # divide by the normalizing value.\n",
        "    return qij\n",
        "</pre>\n",
        "\n",
        "So now we need a way of measuring how different these two distributions are. This is where Kullback-Leibler Divergence comes into play, woohoo!\n",
        "\n",
        "\n",
        "DKL Explainer\n",
        "\n",
        "\n",
        "DKL is a way of asking, given my data (P(i)), how suprised should I be if I found this data (Q(i)).\n",
        "\n",
        "\n",
        "It's given by:\n",
        "\n",
        "$$D_{KL} (P||Q)=\\sum_i P(i)\\log \\frac{P(i)}{Q(i)}\\textrm{}$$\n",
        "\n",
        "In code:\n",
        "\n",
        "<pre>\n",
        "def compute_KLD(P, Q):\n",
        "    KLD = np.sum(P * np.log(P / Q))\n",
        "    return KLD\n",
        "</pre>\n",
        "\n",
        "for Tsne, it looks like\n",
        "\n",
        "\n",
        "$$\\begin{array}{l} C=\\sum_{i\\not= j} p_{ij} \\log \\frac{p_{ij} }{q_{ij} }\\\\  \\end{array}$$\n",
        "\n",
        "What this is saying is compute, for all datapoints the KLD between the low and high dimensional space, except for the diagonals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell and play with the slider to see how when two distributions are far away KLD is high and when they are close it is low."
      ],
      "metadata": {
        "id": "Vng-LrUQax7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cChKruLkv5is"
      },
      "source": [
        "# Define the x-axis\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "\n",
        "# Parameters for P: mean = 2, standard deviation = 1\n",
        "mu_P = 2\n",
        "sigma_P = 1\n",
        "sigma_Q = 1\n",
        "\n",
        "# Define the Gaussian distribution P\n",
        "P = norm.pdf(x, mu_P, sigma_P)\n",
        "\n",
        "def plot_distributions(mu_Q=2.5, sigma_Q=1):\n",
        "    \"\"\"Plot distributions P and Q with varying mu_Q.\"\"\"\n",
        "    # Define the Gaussian distribution Q\n",
        "    Q = norm.pdf(x, mu_Q, sigma_Q)\n",
        "\n",
        "    # Compute KLD\n",
        "    KLD = compute_KLD_display(P, Q)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.fill_between(x, P, color='r', alpha=0.3)\n",
        "    plt.fill_between(x, Q, color='b', alpha=0.3)\n",
        "    plt.plot(x, P, 'r-', linewidth=2)\n",
        "    plt.plot(x, Q, 'b-', linewidth=2)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Probability Density')\n",
        "    titleStr = f'Distributions P and Q with KLD = {KLD:.4f}'\n",
        "    plt.title(titleStr)\n",
        "    plt.legend(['P (True distribution)', 'Q (Approximation)'])\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget with slider for mu_Q\n",
        "widgets.interactive(plot_distributions, mu_Q=(-5, 5, 0.1))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMSU9o8Bv5is"
      },
      "source": [
        "\n",
        "\n",
        "Okay, now that we have a value we can minimize: KLD, how do we do that?\n",
        "\n",
        "\n",
        "It's time for a detour, woohoo!!!\n",
        "\n",
        "\n",
        "Say we have a function:\n",
        "\n",
        "\n",
        "$$F(X,Y)=X^2 +Y^2$$\n",
        "\n",
        "And we want to find the minimum of that function. Well we can go back to calculus and take the concept of the gradient. The gradient of a function is defined as a vector of all the first order partial derivatives of a function. What the gradient tells you is the direction of steepest ascent and how much to care about each variable on your climb.\n",
        "\n",
        "\n",
        "The gradient of a function for multiple variables:\n",
        "\n",
        "\n",
        "$$F(X):X=(x_1 ,x_2 ,\\ldots,x_n )$$\n",
        "\n",
        "$$\\nabla F(X)=\\left(\\frac{\\partial F}{\\partial x_1 },\\frac{\\partial F}{\\partial x_2 },\\ldots,\\frac{\\partial F}{\\partial x_n }\\right)$$\n",
        "\n",
        "Thus the gradient of the function:\n",
        "\n",
        "\n",
        "$$F(X)=x^2 +y^2$$\n",
        "\n",
        "Is:\n",
        "\n",
        "\n",
        "$$\\frac{\\partial F}{\\partial x}=2x$$\n",
        "\n",
        "$$\\frac{\\partial F}{\\partial y}=2y$$\n",
        "\n",
        "$$\\nabla F(X)=(2x,2y)$$\n",
        "\n",
        "The update rule is given by:\n",
        "\n",
        "\n",
        "$$X_{new} =X_{old} -\\alpha \\nabla F(X_{old} )$$\n",
        "\n",
        "Where $\\alpha$ is your learning rate and $\\nabla F(X_{old} )$ is your gradient. This is how big you want your step size to be. It actually looks quite simple in code:\n",
        "\n",
        "<pre>\n",
        "def F(X, Y):\n",
        "    return X**2 + Y**2\n",
        "\n",
        "Alpha = 0.1\n",
        "X_current = -5\n",
        "Y_current = -5\n",
        "Z_current = F(X_current, Y_current)\n",
        "num_iterations = 100\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    X_previous = X_current\n",
        "    Y_previous = Y_current\n",
        "    Z_previous = Z_current\n",
        "\n",
        "    dx = 2 * X_current  # gradient element 1\n",
        "    dy = 2 * Y_current  # gradient element 2\n",
        "\n",
        "    X_current = X_current - Alpha * dx\n",
        "    Y_current = Y_current - Alpha * dy\n",
        "    Z_current = F(X_current, Y_current)\n",
        "\n",
        "\n",
        "</pre>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what it looks like run this cell. You can adjust the parameter alpha and run the cell again to see how the path changes."
      ],
      "metadata": {
        "id": "3GuQBm5hbtgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9L6oS95v5is"
      },
      "source": [
        "alpha = 0.01 #learningrate: play with this to see how the path changes\n",
        "x, y, z, x_trajectory, y_trajectory, z_trajectory = gradientdescentvis(alpha=alpha)\n",
        "\n",
        "def plot_gradient_descent(step=0):\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(x, y, z, cmap=cm.coolwarm, edgecolor='none', alpha=0.7)\n",
        "    ax.view_init(elev=51, azim=42)\n",
        "\n",
        "    ax.plot(x_trajectory[:step+1], y_trajectory[:step+1], z_trajectory[:step+1], 'r-', linewidth=2)\n",
        "    ax.quiver(x_trajectory[step], y_trajectory[step], z_trajectory[step],\n",
        "              x_trajectory[step] - x_trajectory[step-1],\n",
        "              y_trajectory[step] - y_trajectory[step-1],\n",
        "              z_trajectory[step] - z_trajectory[step-1],\n",
        "              color='b', linewidth=2, pivot='tail')\n",
        "\n",
        "    ax.set_xlabel('X-axis')\n",
        "    ax.set_ylabel('Y-axis')\n",
        "    ax.set_zlabel('Cost')\n",
        "    ax.set_title('Gradient Descent')\n",
        "    plt.show()\n",
        "\n",
        "widgets.interactive(plot_gradient_descent, step=(0, len(x_trajectory)-1))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell to display the Gradient Field (gradient evaluated at lots of points on a function) on the function\n",
        "$$F(x,y) = x^2 + y^2 + \\alpha \\sin(\\beta x) + \\alpha \\sin(\\beta y)$$\n",
        "\n",
        "where $\\alpha$ represents the amplitude of the sin wave and $\\beta$ represents the sin multiplier.\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = 2x + \\beta \\alpha \\cos(\\beta x)$$\n",
        "$$\\frac{\\partial z}{\\partial y} = 2y + \\beta \\alpha \\cos(\\beta y)$$\n",
        "\n",
        "$$\\nabla F(x,y) = \\left[ \\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y} \\right] = \\left[ 2x + \\beta \\alpha \\cos(\\beta x), 2y + \\beta \\alpha \\cos(\\beta y) \\right]$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1lMlerCBb5y8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VwvMxiFv5is"
      },
      "source": [
        "plot_gradient_field()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf5r4WiXv5is"
      },
      "source": [
        "\n",
        "Okay so now we know what gradient descent is. Let's put it all together. In practice the gradient of the cost function for tsne is given by:\n",
        "\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial y_i }=4\\sum_j (p_{ij} -q_{ij} )(y_i -y_j )(1+||y_i -y_j ||^2 )^{-1}$$\n",
        "\n",
        "\n",
        "This can be interpeted as consisting of an attractive and a repulsive force. If Pij is higher than Qij, than the force exerted on that point, i, will be attractive, because the affinities are bigger in high than low. The repulsive force is exterted in the opposite way.\n",
        "\n",
        "\n",
        "The attractive term:\n",
        "\n",
        "\n",
        "$$(p_{ij} )(y_i -y_j )$$\n",
        "\n",
        "The repulsive term:\n",
        "\n",
        "\n",
        "$$(q_{ij} )(y_i -y_j )$$\n",
        "\n",
        "These are subtracted to get a total force exerted.\n",
        "\n",
        "\n",
        "The final term in the equation comes from the definition of the T distribution and determines how much to care about each point.\n",
        "\n",
        "\n",
        "As the distance $||y_i -y_j ||$ between two points increases, the term $(1+||y_i -y_j ||^2 )^{-1}$ approaches zero. This ensures that algorithim tracks local structure in the high dimensional space.\n",
        "\n",
        "\n",
        "Finally! Let's see it in action and how it behaves with different parameters.\n",
        "\n",
        "\n",
        "Remember: There are four columns: Sepal length, Sepal width, Petal length, Petal width. One flower (observation) per row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB9QbtdDv5is"
      },
      "source": [
        "# Load the Fisher Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "# Create sliders\n",
        "perplexity_slider = widgets.FloatSlider(value=30, min=1, max=len(y)-1, step=1, description='Perplexity:')\n",
        "exaggeration_slider = widgets.FloatSlider(value=3, min=1, max=1000, step=0.5, description='Exaggeration:')\n",
        "alpha_slider = widgets.FloatSlider(value=190, min=1, max=1000, step=10, description='Alpha:')\n",
        "\n",
        "# Create interactive widget\n",
        "widgets.interactive(update_tsne, perplexity=perplexity_slider, exaggeration=exaggeration_slider, alpha=alpha_slider)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dk0kEeco3qHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}