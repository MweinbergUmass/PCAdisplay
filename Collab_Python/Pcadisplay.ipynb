{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MweinbergUmass/Teach_Dimred/blob/main/Collab_Python/Pcadisplay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For newbies, all you have to do should be to press the run button on the side of each cell too see the output of the code blocks.\n",
        "\n",
        "First run the cells for installing and importing the nescessary dependencies."
      ],
      "metadata": {
        "id": "fOU75Mva81Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mpl_interactions pca"
      ],
      "metadata": {
        "id": "cs49h3qar-1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mpl_interactions.ipyplot as iplt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, load_digits\n",
        "from pca import pca as dooppypca\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "AHX6RpZOrvSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWpFT_p7p1uE"
      },
      "source": [
        "\n",
        "Disclaimer: I wrote this originally all in a matlab live editor so there might be some residual errors particularly in the code for display.\n",
        "\n",
        "In order to talk about PCA there are a few concepts we should get under our belt first.\n",
        "\n",
        "\n",
        "Specifically, Variance and Covariance.\n",
        "\n",
        "\n",
        "Lets start with variance, which makes for an easy generalization to covariance.\n",
        "\n",
        "\n",
        "IMPORTANT NOTE: When we use dimensionality reduction techniques, it is helpful to think about our columns as features and our rows as data points.\n",
        "\n",
        "\n",
        "$$var(X)=\\sum_{i=1}^n \\frac{1}{n-1}(X_i -\\mu )^2$$\n",
        "\n",
        "Where $\\mu$ is the sample mean.\n",
        "\n",
        "\n",
        "In code:\n",
        "\n",
        "\n",
        "The scalar of n-1 comes from what's called Bessel correction, it's a technicality so don't worry too much about it. (Its for when you only have a sample and not the full population)\n",
        "\n",
        "<pre>\n",
        "def compute_variance(X):\n",
        "    n = X.shape[1]\n",
        "    mu = np.mean(X, axis=0)\n",
        "    scaler = 1 / (X.shape[0] - 1)\n",
        "    variance = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        variance[i] = scaler * np.sum((X[:, i] - mu[i]) ** 2)\n",
        "    return variance\n",
        "\n",
        "</pre>\n",
        "\n",
        "Disclaimer 2: In practice you should always use numpy or another trusted library to compute variance. Their operations will be much faster. For example:\n",
        "\n",
        "<pre>\n",
        "var = np.var(X)\n",
        "</pre>\n",
        "\n",
        "\n",
        "Here's a plot that showcases what variance looks like. As you can see it is the \"spread\" of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17hA3GIp1uG"
      },
      "source": [
        "#Initializing my variables\n",
        "N = 100\n",
        "mu = [0, 0]\n",
        "variances = [0.5, 1, 2, 5, 7.5, 10]\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for i, var in enumerate(variances):\n",
        "    # generating random points from a 2 dimensional gaussian\n",
        "    sigma = np.array([[var, 0], [0, var]])\n",
        "    points = np.random.multivariate_normal(mu, sigma, N)\n",
        "\n",
        "    # plotting\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.scatter(points[:, 0], points[:, 1], s=5)\n",
        "    plt.title(f'Variance = {var}')\n",
        "    plt.axis('equal')\n",
        "    plt.grid(True)\n",
        "    plt.xlim([-10, 10])\n",
        "    plt.ylim([-10, 10])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwevZcpUp1uH"
      },
      "source": [
        "\n",
        "Our data can also vary in multiple directions (adjust the sliders):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dMuzz-Up1uH"
      },
      "source": [
        "N = 50\n",
        "mu = [0, 0]\n",
        "#function for plotting points based on the variance\n",
        "def plot_points(varx, vary):\n",
        "    sigma = np.array([[varx, 0], [0, vary]])\n",
        "    points = np.random.multivariate_normal(mu, sigma, N)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(points[:, 0], points[:, 1], s=5)\n",
        "    plt.xlim([-10, 10])\n",
        "    plt.ylim([-10, 10])\n",
        "    plt.title(f\"Variance x: {varx}, Variance y: {vary}\")\n",
        "    plt.show()\n",
        "\n",
        "widgets.interactive(plot_points,\n",
        "                    varx=widgets.FloatSlider(value=1, min=1, max=100, step=1, description='Var x:'),\n",
        "                    vary=widgets.FloatSlider(value=1, min=1, max=100, step=1, description='Var y:'))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt_HSFCop1uH"
      },
      "source": [
        "\n",
        "We can also calculate how different variables covary, or vary together. The formula for covariance is very similar:\n",
        "\n",
        "\n",
        "$$Cov(X,Y)=\\sum_{i=1}^n \\frac{1}{n-1}(X_i -X_{\\mu } )(Y_i -Y_{\\mu } )$$\n",
        "\n",
        " In code:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<pre>\n",
        "def Cov(X):\n",
        "    n, m = X.shape  # n = number of data points, m = features\n",
        "    scalar = 1 / (n - 1)  # Bessel correction\n",
        "    cov_matrix = np.zeros((m, m))  # initialize covariance matrix\n",
        "    X_mu = np.mean(X, axis=0)  # mean of each feature in X\n",
        "    \n",
        "    for i in range(m):\n",
        "        T_1 = X[:, i] - X_mu[i]  # term 1 doesn't need to be iterated in the inner loop\n",
        "        for j in range(m):\n",
        "            cov_matrix[i, j] = np.sum(T_1 * (X[:, j] - X_mu[j]))  # for each other variable\n",
        "            cov_matrix[i, j] = cov_matrix[i, j] * scalar\n",
        "            \n",
        "    return cov_matrix\n",
        "\n",
        "#output is an m by m matrix where the diagonal is the variance of each variable\n",
        "#and the off diagonal is how variable i covary's with variable j.\n",
        "\n",
        "</pre>\n",
        "\n",
        "Disclaimer 3: Same as before, use something like:\n",
        "<pre>\n",
        "cov_np = np.cov(X, rowvar=False) #Our columns are variables.\n",
        "</pre>\n",
        "\n",
        "\n",
        "As you can see, in the case where X and Y are the same this just becomes the formula for variance:\n",
        "\n",
        "\n",
        "$$var(X)=\\sum_{i=1}^n \\frac{1}{n-1}(X_i -X_{\\mu } )^2$$\n",
        "\n",
        "When we use PCA we make use of the covariance matrix:\n",
        "\n",
        "\n",
        "$$S=w^T w$$\n",
        "\n",
        "The covariance matrix is defined as a matrix where the nth element gives the covariance between datapoint i and variable j.\n",
        "\n",
        "\n",
        "The diagonal of $S$ is simply the variance of the ith element.\n",
        "\n",
        "\n",
        "To see covariance in action lets take a look at these plots:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-z5HCdRp1uI"
      },
      "source": [
        "# Seting the seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "N = 50\n",
        "\n",
        "# positive cov data\n",
        "X_pos = np.linspace(0, 10, N)\n",
        "Y_pos = X_pos + 5 + np.random.randn(N)\n",
        "\n",
        "# negative cov data\n",
        "X_neg = np.linspace(0, 10, N)\n",
        "Y_neg = -X_neg + np.random.randn(N)\n",
        "\n",
        "# Generate data with no (or very little) covariance\n",
        "X_none = np.linspace(0, 10, N)\n",
        "Y_none = 5 + np.random.randn(N)\n",
        "\n",
        "# plotting\n",
        "plt.figure()\n",
        "plt.scatter(X_pos, Y_pos, c='b', linewidth=2)\n",
        "plt.xlim([0, 10])\n",
        "plt.ylim([0, 20])\n",
        "plt.grid(True)\n",
        "covmat_pos = np.cov(X_pos, Y_pos)\n",
        "plt.title(f\"Positive Covariance: Cov(X, Y) = {covmat_pos[0,1]:.2f}\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_neg, Y_neg, c='r', linewidth=2)\n",
        "plt.xlim([0, 10])\n",
        "plt.ylim([-10, 10])\n",
        "plt.grid(True)\n",
        "covmat_neg = np.cov(X_neg, Y_neg)\n",
        "plt.title(f\"Negative Covariance: Cov(X, Y) = {covmat_neg[0,1]:.2f}\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_none, Y_none, c='g', linewidth=2)\n",
        "plt.xlim([0, 10])\n",
        "plt.ylim([0, 10])\n",
        "plt.grid(True)\n",
        "covmat_none = np.cov(X_none, Y_none)\n",
        "plt.title(f\"No Covariance: Cov(X, Y) = {covmat_none[0,1]:.2f}\", fontsize=14)\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK3aAHf7p1uI"
      },
      "source": [
        "\n",
        "Let's go back to our original data set where we set the variance in the x and y direction and let's see if we can find the line that maximizes the variance. We will then use this line as part of our new coordinate system. The values of our data will be given by their projections (I'll go over this in a second) on the line. This makes for an optimal representation because the direction of maximal variance is often the one where information is lies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvALmGefp1uI"
      },
      "source": [
        "# Generating some points with positive covariance\n",
        "n_points = 50\n",
        "x_values = np.random.uniform(-10, 10, n_points)\n",
        "m = 1.5\n",
        "b = 0\n",
        "noise = np.random.normal(0, 2, n_points)\n",
        "y_values = m * x_values + b + noise\n",
        "points = np.column_stack((x_values, y_values))\n",
        "\n",
        "def plot_with_slope(m):\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    plt.scatter(points[:, 0], points[:, 1], color='blue', linewidth=2)\n",
        "\n",
        "    xLims = np.array([-10, 10])\n",
        "    plt.xticks(np.linspace(-10,10,5))\n",
        "    plt.xlim([-10,10])\n",
        "    plt.ylim([-10,10])\n",
        "\n",
        "    b = 0\n",
        "    plotline(m, xLims, b)\n",
        "\n",
        "    # Direction vector for the line\n",
        "    u = np.array([1, m])\n",
        "    u = u / np.linalg.norm(u)  # normalize vector\n",
        "\n",
        "    # project points and plot line\n",
        "    for point in points:\n",
        "        v = point\n",
        "        proj = (np.dot(v, u) / np.dot(u, u)) * u\n",
        "        plt.plot([point[0], proj[0]], [point[1], proj[1]], 'r--')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plotline(m, x, b):\n",
        "    y = m*x + b\n",
        "    plt.plot(x, y, linewidth=2)\n",
        "\n",
        "\n",
        "widgets.interactive(plot_with_slope, m=widgets.FloatSlider(value=-10, min=-20, max=20, step=0.5))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9sq5iHp1uI"
      },
      "source": [
        "\n",
        "In order to find the line that maximizes the variance PCA looks for the line that minimizes the sum of perpendicular square distances to the line.\n",
        "Hopefully as you saw by playing with the slider, you get the intuiton that this is the line that maximizes the variance.\n",
        "\n",
        "Lets take a look at exactly what I mean. Here you can play with the sliders which will adjust the variance in the x and y direction and plot the corresponding principal component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GmU114rp1uI"
      },
      "source": [
        "def plot_with_variances(variancex, variancey):\n",
        "    sigma = np.array([[variancex, 0], [0, variancey]])\n",
        "\n",
        "    # unimportant arbitrary choices for these values\n",
        "    mu = np.array([0, 0])\n",
        "    N = 50\n",
        "    points = np.random.multivariate_normal(mu, sigma, N)\n",
        "\n",
        "    plt.scatter(points[:, 0], points[:, 1], color='blue')\n",
        "    plt.xlim([-10, 10])\n",
        "    plt.ylim([-10, 10])\n",
        "\n",
        "    #Compute pc's\n",
        "    matforproj_centered = points - np.mean(points, axis=0)\n",
        "    matforprojlater_covarmat = np.dot(matforproj_centered.T, matforproj_centered) / (N-1)\n",
        "    eig_vals, V = np.linalg.eig(matforprojlater_covarmat)\n",
        "    idx = eig_vals.argsort()[::-1]\n",
        "    V = V[:, idx]\n",
        "    percent_var_explained = sorted(eig_vals, reverse=True) / np.sum(eig_vals) * 100\n",
        "\n",
        "    # Plot the first principal component\n",
        "    t = np.linspace(min(np.concatenate([plt.xlim(), plt.ylim()])),\n",
        "                    max(np.concatenate([plt.xlim(), plt.ylim()])), 50)\n",
        "    line = np.mean(points, axis=0) + np.outer(t, V[:,0])\n",
        "    plt.plot(line[:, 0], line[:, 1], '-r', linewidth=2)\n",
        "    xLims = plt.xlim()\n",
        "    yLims = plt.ylim()\n",
        "    xPos = xLims[1] - 0.10 * (xLims[1] - xLims[0])\n",
        "    yPos = yLims[1] - 0.05 * (yLims[1] - yLims[0])\n",
        "    plt.text(xPos, yPos, f\"PC1: {percent_var_explained[0]:.2f}%\", color='red', fontsize=12, horizontalalignment='right')\n",
        "\n",
        "    a = np.mean(points, axis=0)\n",
        "    v = V[:,0]\n",
        "\n",
        "    #projecting\n",
        "    for i in range(N):\n",
        "        p = points[i, :]\n",
        "        q = a + np.dot(p - a, v) / np.dot(v, v) * v\n",
        "        plt.plot([p[0], q[0]], [p[1], q[1]], 'r--')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "widgets.interactive(plot_with_variances,\n",
        "                    variancex=widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1),\n",
        "                    variancey=widgets.FloatSlider(value=4.4, min=0.1, max=10, step=0.1))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1tNGPU5p1uJ"
      },
      "source": [
        "\n",
        "PCA computes a direction (vector) for each original dimension of the data. Each line or hyperplane explains the next most variance, after the one before it and is orthogonal to the last. The vectors being orthogonal means that they each capture the next most variance. To see that we can vary the spread of the x and y data seperately and watch how each of the pc's adjust accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyUacOpVp1uJ"
      },
      "source": [
        "#function for interactive plotting of pcs based on var in dim 2\n",
        "def plot_with_variances(variancex, variancey):\n",
        "\n",
        "    #init vars\n",
        "    mu = np.array([0, 0])\n",
        "    N = 50\n",
        "    sigma = np.array([[variancex, 0], [0, variancey]])\n",
        "    points = np.random.multivariate_normal(mu, sigma, N)\n",
        "\n",
        "    #Compute PC1 and 2\n",
        "    matforproj_centered = points - np.mean(points, axis=0)\n",
        "    matforprojlater_covarmat = np.dot(matforproj_centered.T, matforproj_centered) / (N-1)\n",
        "    eig_vals, V = np.linalg.eig(matforprojlater_covarmat)\n",
        "    idx = eig_vals.argsort()[::-1]\n",
        "    V = V[:, idx]\n",
        "    percent_var_explained = sorted(eig_vals, reverse=True) / np.sum(eig_vals) * 100\n",
        "\n",
        "    # Plotting\n",
        "    plt.scatter(points[:, 0], points[:, 1], color='blue')\n",
        "    t = np.linspace(min(np.concatenate([plt.xlim(), plt.ylim()])),\n",
        "                    max(np.concatenate([plt.xlim(), plt.ylim()])), 50)\n",
        "    line = np.mean(points, axis=0) + np.outer(t, V[:,0])\n",
        "    plt.plot(line[:, 0], line[:, 1], '-r', linewidth=2)\n",
        "    line2 = np.mean(points, axis=0) + np.outer(t, V[:,1])\n",
        "    plt.plot(line2[:, 0], line2[:, 1], '-g', linewidth=2)\n",
        "\n",
        "    xLims = plt.xlim()\n",
        "    yLims = plt.ylim()\n",
        "\n",
        "    xPos = xLims[1] - 0.10 * (xLims[1] - xLims[0])\n",
        "    yPos1 = yLims[1] - 0.05 * (yLims[1] - yLims[0])\n",
        "    yPos2 = yPos1 - 0.1 * (yLims[1] - yLims[0])\n",
        "\n",
        "    plt.text(xPos, yPos1, f\"PC1: {percent_var_explained[0]:.2f}%\", color='red', fontsize=12, horizontalalignment='right')\n",
        "    plt.text(xPos, yPos2, f\"PC2: {percent_var_explained[1]:.2f}%\", color='green', fontsize=12, horizontalalignment='right')\n",
        "    plt.title(f\"Variance X = {variancex} Variance Y = {variancey}\")\n",
        "    plt.xlim(xLims)\n",
        "    plt.ylim(yLims)\n",
        "    plt.show()\n",
        "\n",
        "widgets.interactive(plot_with_variances,\n",
        "                    variancex=widgets.FloatSlider(value=100, min=0.1, max=200, step=0.1),\n",
        "                    variancey=widgets.FloatSlider(value=100, min=0.1, max=200, step=0.1))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBqUc62Ep1uJ"
      },
      "source": [
        "\n",
        "The final concept that is important to understand for pca is projection. The formula for a projection of a point $u\\;$onto a point $v$ is given by:\n",
        "\n",
        "\n",
        "$${\\textrm{proj}}_u (v)=\\frac{v\\cdot u}{u\\cdot u}u$$\n",
        "\n",
        "Where $v\\cdot u$ is the dot product of $v$ onto $u$ and $u\\cdot u$ is the magnitude of $u$ squared and you scale in the direction of $u$ .\n",
        "\n",
        "\n",
        "In practice you don't have to use the formula with PCA because it is implicit in the matrix multiplication with the eigenvectors.\n",
        "\n",
        "\n",
        "An intuitive way of thinking about it is if you have a point and a line and you project the point onto the line the projection is the shadow cast by the point on the line. Here's a visual example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpy03fgWp1uJ"
      },
      "source": [
        "# Define line and point\n",
        "u = np.array([4, 3])  # line\n",
        "v = np.array([2, 4])  # Point\n",
        "\n",
        "#projecting\n",
        "proj_v_onto_u = (np.dot(v, u) / np.dot(u, u)) * u\n",
        "proj_u_onto_v = (np.dot(u, v) / np.dot(v, v)) * v\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', linewidth=2)\n",
        "plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', linewidth=2)\n",
        "plt.quiver(0, 0, proj_v_onto_u[0], proj_v_onto_u[1], angles='xy', scale_units='xy', scale=1, color='g', linewidth=2)\n",
        "plt.plot([u[0], proj_u_onto_v[0]], [u[1], proj_u_onto_v[1]], 'k--', linewidth=1.5)\n",
        "plt.legend(['Vector u (line)', 'Vector v (point)', 'Projection of v onto u', 'u to its projection on v'])\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.title('Projection of vector v onto vector u')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XKdo4icp1uJ"
      },
      "source": [
        "\n",
        "Great! Now lets look at the output of PCA on the fisher iris dataset.\n",
        "\n",
        "\n",
        "3 Plots:\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; 1: Scatter: These are your scores (projections in the new space). Ideally you'll see differentiation here.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; 2: Scree plot: This is a bar graph where the x is each pc and the y is explained variance.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; 3: Biplot: The arrows are the variables(very meanignful) and the dots are the scores.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - The arrows are given by your coefficients matrix. If you have 4 variables it will be 4x4. Your first row is your first  variables contribution to each pc and so on.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - The angle between arrows represents the correlation between variables. Orthogonal = uncorrelated; obtuse = anti-correlated, acute = correlated.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - The magnitude of your arrows and the direction they face correspond to how much each of them contributes to each pc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A9MD5Frp1uK"
      },
      "source": [
        "#Using non standard pca library here for easier biplots\n",
        "# Load the iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data['data']\n",
        "y = data['target']\n",
        "species = data['target_names'][y]\n",
        "columns_names = data['feature_names']\n",
        "dframe = pd.DataFrame(X, columns=columns_names, index=y)\n",
        "model = dooppypca(n_components=0.95)\n",
        "model = dooppypca(n_components=2)\n",
        "model = dooppypca(verbose=0)\n",
        "\n",
        "results = model.fit_transform(X=dframe)\n",
        "fig, ax = model.biplot(n_feat=4, verbose=0,figsize=(15,10))\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "#fixing the legend\n",
        "species_names = data.target_names\n",
        "label_dict = {str(i): species_name for i, species_name in enumerate(species_names)}\n",
        "modified_labels = [label_dict[label] if label in label_dict else label for label in labels]\n",
        "ax.legend(handles, modified_labels)\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the biplot, it looks like petal width and length are highly correlated, let's see if that's true."
      ],
      "metadata": {
        "id": "4uFaY89H3iX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSMz73-Yp1uK"
      },
      "source": [
        "petal_length = X[:, 2]  # petal length is the third column\n",
        "petal_width = X[:, 3]   # petal width is the fourth column\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "for i, species in enumerate(species_names):\n",
        "    plt.scatter(petal_length[y == i],\n",
        "                petal_width[y == i],\n",
        "                label=species,\n",
        "                color=colors[i],\n",
        "                edgecolor='black',\n",
        "                s=50)\n",
        "\n",
        "\n",
        "plt.xlabel('Petal Length (cm)', fontsize=14)\n",
        "plt.ylabel('Petal Width (cm)', fontsize=14)\n",
        "plt.title('Petal Width vs Petal Length', fontsize=16)\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the Scree plot"
      ],
      "metadata": {
        "id": "iqUutyiM3ffK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-duA0Rpp1uK"
      },
      "source": [
        "fig, ax = model.plot(figsize=(10, 5))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMBzyNjop1uK"
      },
      "source": [
        "When using PCA it's important to keep in mind some of it's limitations and advantages. PCA can only capture linear relationships in data because it represents each data point using a linear combination of basis vectors (the eigenvectors of the covariance matrix). Often times (most of the time), the data we're interested is not linearly seperable. Some advantageous of PCA are that it's incredibly fast, and that it's dimensions are often fairly interpretable.\n",
        "\n",
        "To see how it fairs against data that is not linearly seperated easily let's compare it's performance on a rolled up data set against a non linear technique, T-Distributed Stochastic Neighbor Embedding (TSNE). Tsne might be very slow, this is a major disadvantage of tsne with big data sets.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC57RX1Xp1uK"
      },
      "source": [
        "# Generate Swiss Roll dataset, hehe\n",
        "N = 1500\n",
        "t = np.linspace(0, 3 * np.pi, N)\n",
        "X = np.vstack((t * np.cos(t), t * np.sin(t), 0.5 * np.random.rand(N))).T\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "tsne = TSNE(perplexity=100, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# plotting stuff\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax1 = plt.subplot(1, 3, 1, projection='3d')\n",
        "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.jet, s=20)\n",
        "ax1.set_title('Original Swiss Roll')\n",
        "ax1.view_init(15, -20)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=t, cmap=plt.cm.jet, s=20)\n",
        "plt.title('PCA Projection')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=t, cmap=plt.cm.jet, s=20)\n",
        "plt.title('t-SNE Projection')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the difference even clearer let's take a dataset with very complex nonlinear relationships, the mnist handwritten digits dataset. This dataset consists of 28x28 pixel grayscale images of handwritten digits (0 through 9), and the goal is often to classify each image according to the digit it represents. So each data point is a 784 (28*28) dimensional vector which we want to reduce down to 2 dimensions and see if we can spot the differences.\n",
        "\n",
        "Let's take a look at what the dataset looks like first."
      ],
      "metadata": {
        "id": "-T4jYOWB48Xc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JI9ALe5p1uK"
      },
      "source": [
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "images = digits.images\n",
        "\n",
        "# a random 9 digits\n",
        "plt.figure(figsize=(8, 8))\n",
        "indices = np.random.choice(len(X), 9)\n",
        "\n",
        "for i, index in enumerate(indices):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[index], cmap='gray')\n",
        "    plt.title(f\"Digit: {y[index]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNOnvbn4p1uK",
        "collapsed": true
      },
      "source": [
        "#using pca and tsne  on the digits dataset\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "tsne = TSNE(perplexity=30, n_components=2, init='pca', random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "#plotting stuff\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.jet, edgecolor='k', s=40)\n",
        "plt.colorbar()\n",
        "plt.title('PCA Projection')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=plt.cm.jet, edgecolor='k', s=40)\n",
        "plt.colorbar()\n",
        "plt.title('t-SNE Projection')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE93SnE-p1uK"
      },
      "source": [
        "\n",
        "Appendix. (Warning: this is a lot of pretty formal (but not too formal) math. I also assume you are familiar with some linear algebra, some calculus, and some matrix calculus identities. However if you don't know those things, I encourage you to try and read on and take in the main concepts which I explain in words.)\n",
        "\n",
        "\n",
        "We start by stating our problem, we want to find the vector which explains the most possible variance. Therefore our first step is to find the vector $w$ that maximizes the variance of our data matrix $X$ when it is projected onto it.\n",
        "\n",
        "\n",
        "We can express the variance in a direction $w$ as ${w^T Sw}$ . Where $S$ is our mean centered covariance matrix. This is a really cool linear algebra trick that express a seemingly abstract concept in a formal way. To see that lets look at what exactly this operation is doing.\n",
        "\n",
        "\n",
        " We do ${Sw}$ first, this will end up as an $M\\times 1$ weighted combination of our variance. It can be useful to remember that matrix multiplication is a transformation that in this case scales, shifts, or stretches it depending on the variance. Now for the magic, when we multiply ${w^T }$ by ${Sw}$ we get out a scalar, this projection tells us how much of ${Sw}$ is \"in the direction of $w$ \". In other words, ${w^T Sw}$ tells us how much variance there is in the direction of $w$.\n",
        "\n",
        "\n",
        "\n",
        "Okay, so the objective is to find the direction that maximizes the variance of our data which we can represent as ${w^T Sw}$ . Now to introduce a constraint, we want $w$ to be a unit vector, or formally we want ${w^T w}=1$ . We want $w$ to be a unit vector because otherwise there would be infinitely many solutions because you could just scale $w$ to be infinitely long to capture \"more and more\" of the variance. You could of course pick some other constant besides 1 to be the length of our vector, but 1 is simple and it leads to this awesome solution.\n",
        "\n",
        "\n",
        "Given our constraint ${w^T w}=1$ and the objective $w^* =argmax_w w^T Sw$ we have a constrained optimization problem. We can unconstrain this problem using lagrange multipliers, a fancy tool I'd never heard of until recently and they seem a bit like magic. Lagrange multipliers are typically written like:\n",
        "\n",
        "\n",
        "$$L(w,\\lambda )=Objective-\\lambda (Constraint-1)$$\n",
        "\n",
        "This gives us:\n",
        "\n",
        "\n",
        "$$L(w,\\lambda )=w^T Sw-\\lambda (w^T w-1)$$\n",
        "\n",
        "The intuiton here is that $\\lambda$ provides us with a penalty when $w^T w$ is far away from 1. This constraint is enforced when we take the derivatives of $L$ with respect to $\\lambda$ and set it to zero. We will do the same for $w$ to find the extrema of our function.\n",
        "\n",
        "\n",
        "The gradient of $L$ with respect to $w$ is as follows:\n",
        "\n",
        "\n",
        " Differentiating the term ${w^T Sw}$ :\n",
        "\n",
        "\n",
        "$$\\frac{\\partial }{\\partial w}(w^T Sw)=2Sw$$\n",
        "\n",
        "For the term $\\lambda w^T w$ :\n",
        "\n",
        "\n",
        "$$\\frac{\\partial }{\\partial w}(\\lambda w^T w)=2\\lambda w$$\n",
        "\n",
        "Putting those together:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w}=2Sw-2\\lambda w$$\n",
        "\n",
        "Rearranging we get:\n",
        "\n",
        "\n",
        "$$Sw=\\lambda w$$\n",
        "\n",
        "Does this look familiar??????? It's okay if it doesn't but if it does I hope that was cool, it's an eigenvalue problem! Our eigenvector is $w$ and our eigenvalue is $\\lambda$! Thus, the direction that explains the most variance is the eigenvector of our covariance matrix. I won't break it all down but one way to then find the next eigenvectors which explain the next most variance and is orthogonal to the last is through a method called deflation.\n",
        "\n",
        "\n",
        "Essentially it works by computing the outer product of $w$ , $P={{ww}}^T$ , then \"deflating\":\n",
        "\n",
        "\n",
        "$$S_{deflated} =S-\\lambda P$$\n",
        "\n",
        "then solving for the eigenvectors and values of $S_{deflated}$ and so on.\n",
        "\n",
        "\n",
        "Oh and don't think I forgot about The gradient of $L$ with respect to $\\lambda$ , it works out to just be our original constraint which is what we want (thats the beauty of lagrange multipliers).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}